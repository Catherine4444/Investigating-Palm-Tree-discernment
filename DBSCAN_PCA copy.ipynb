{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBSCAN clustering \n",
    "import numpy as np\n",
    "from sklearn.cluster import  DBSCAN\n",
    "from sklearn.metrics import confusion_matrix, f1_score, normalized_mutual_info_score, precision_score, recall_score\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the method specifies key to make sure the ground truth set is in consistent order as the valeus accessed \n",
    "def ground_truth_set(key):\n",
    "    ground_truth = []\n",
    "    for item in key:\n",
    "        if \"Palm\" in item: \n",
    "            ground_truth.append(1)\n",
    "        else:\n",
    "            ground_truth.append(0)\n",
    "    \n",
    "    return np.array(ground_truth)\n",
    "\n",
    "def data_set(key, dict):\n",
    "    data = []\n",
    "    for item in key:\n",
    "        data.append(dict[item])\n",
    "    return np.array(data)\n",
    "\n",
    "def get_truth_and_data(fileName):\n",
    "    dict = np.load(fileName, allow_pickle=True).item()\n",
    "    dict_keys = dict.keys()\n",
    "    print(len(dict_keys))\n",
    "    ground_truth = ground_truth_set(dict_keys)\n",
    "    data_values = data_set(dict_keys, dict)\n",
    "\n",
    "    return ground_truth, data_values\n",
    "\n",
    "def accuracy(truth, pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(truth, pred, labels=[0,1]).ravel()\n",
    "    return (tn+tp)/ (tn+ fp+ fn+ tp)\n",
    "\n",
    "def three_d_accuracy(truth, pred):\n",
    "    matrix1 = confusion_matrix(truth, pred, labels=[-1,0,1])\n",
    "    \n",
    "    trues1 = matrix1[0][0]+matrix1[1][1]+matrix1[2][2]\n",
    "    accuracy_percent1 = trues1/len(truth)\n",
    "\n",
    "    #see if swapped around is better \n",
    "    mapping = {-1:-1, 0: 1, 1: 0}\n",
    "    # Reassign the labels using the mapping\n",
    "    new_pred = np.array([mapping[label] for label in pred])\n",
    "\n",
    "    matrix2 = confusion_matrix(truth, new_pred, labels=[-1,0,1])\n",
    "    \n",
    "    trues2 = matrix2[0][0]+matrix2[1][1]+matrix2[2][2]\n",
    "    accuracy_percent2 = trues2/len(truth)\n",
    "\n",
    "    if (accuracy_percent1 > accuracy_percent2):\n",
    "        print(matrix1)\n",
    "        # print(\"1. \", accuracy_percent1)\n",
    "        return accuracy_percent1\n",
    "    # print(\"2. \", accuracy_percent2)\n",
    "    print(matrix2)\n",
    "    return accuracy_percent2\n",
    "\n",
    "def dbscan_labels(e, dataset):\n",
    "    minpt = int(len(dataset)*0.025)\n",
    "    print(\"chosen minpt is 0.1 of sample size \", minpt)\n",
    "    print(\"the chosen eps: \", e)\n",
    "    dbscan = DBSCAN(eps = e, min_samples=minpt)\n",
    "    labels = dbscan.fit_predict(dataset)\n",
    "    labels = np.array(labels)\n",
    "    unique = np.unique(labels)\n",
    "    result = np.sum(labels == -1)\n",
    "    print(\"The unique labels are: \", unique)\n",
    "    print(\"the amount of noisy samples are: \", result)\n",
    "    return labels\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "def knn_plot(data):\n",
    "    nn = int(len(data)*0.025)-1\n",
    "    neighbors = NearestNeighbors(n_neighbors=nn)\n",
    "    neighbors_fit = neighbors.fit(data)\n",
    "    distances, indices = neighbors_fit.kneighbors(data)\n",
    "\n",
    "    distances = np.sort(distances[:, -1])  # Get the distances to the 4th nearest neighbor\n",
    "\n",
    "    plt.plot(distances)\n",
    "    plt.xlabel(\"Points sorted by distance\")\n",
    "    plt.ylabel(\"Distance to 4th nearest neighbor\")\n",
    "    plt.title(\"k-NN Distance Plot\")\n",
    "    plt.show()\n",
    "\n",
    "def experiment(e, truth, data):\n",
    "    labels = dbscan_labels(e, data)\n",
    "    precision = precision_score(truth, labels, labels = [0,1], average = 'weighted')\n",
    "    recall = recall_score(truth, labels, labels = [0,1], average = 'weighted')\n",
    "    f1 = f1_score(truth, labels, labels = [0,1], average = 'weighted')\n",
    "    return precision, recall, f1 \n",
    "\n",
    "import csv \n",
    "def record_results(e, truth, data, dict, name, experi =\"dbscan\"): \n",
    "    precision, recall, f1 = experiment(e, truth,data)\n",
    "    if name not in dict:\n",
    "        dict[name] = [precision, recall, f1]\n",
    "\n",
    "    if not os.path.exists(f\"results_{experi}/{name}.csv\"):\n",
    "        with open(f\"results_{experi}/{name}.csv\", 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow((\"precision\",\"recall\", \"f1\"))\n",
    "            writer.writerow((precision, recall, f1))\n",
    "    else:\n",
    "        with open(f\"results_{experi}/{name}.csv\", 'a', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow((precision, recall, f1)) \n",
    "    \n",
    "    print(name)\n",
    "    if dict[name][0] <= precision:\n",
    "        dict[name][0] = precision\n",
    "    else:\n",
    "        print(\"max precision: \", dict[name][0])\n",
    "\n",
    "    if dict[name][1] <= recall:\n",
    "        dict[name][1] = recall\n",
    "    else:\n",
    "        print(\"max recall: \", dict[name][0])\n",
    "\n",
    "    if dict[name][2] <= f1:\n",
    "        dict[name][2] = f1\n",
    "    else:\n",
    "        print(\"max f1 : \", dict[name][2])\n",
    "    \n",
    "    print(\"f1 : \", f1 , \"\\nprecision: \", precision,\"\\nrecall: \", recall)\n",
    "    \n",
    "def experiment_eps(title, eps_array, truth, dataset):\n",
    "    print(title)\n",
    "    for i in eps_array:\n",
    "        labels = dbscan_labels(i, dataset)\n",
    "        arr = np.array(labels)\n",
    "        unique = np.unique(labels)\n",
    "        result = np.sum(arr == -1)\n",
    "        \n",
    "        if(len(unique) == 3 or len(unique) == 2 or len(unique) == 4 or len(unique) == 5):\n",
    "            f1 = f1_score(truth, labels, labels = [0,1], average = 'weighted')\n",
    "            print(\"F1 score: \", f1)\n",
    "            # print(\"accuracy: \", three_d_accuracy(truth, labels))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glcm_truth_n_pca, glcm_data_n_pca =  get_truth_and_data('GLCM_n_pca.npy')\n",
    "glcm_truth_pca, glcm_data_pca =  get_truth_and_data('GLCM_pca.npy')\n",
    "\n",
    "hist_truth_n_pca, hist_data_n_pca =  get_truth_and_data('hist_n_pca.npy')\n",
    "hist_truth_pca, hist_data_pca =  get_truth_and_data('hist_pca.npy')\n",
    "\n",
    "hog_truth_n_pca, hog_data_n_pca =  get_truth_and_data('hog_n_pca.npy')\n",
    "hog_truth_pca, hog_data_pca =  get_truth_and_data('hog_pca.npy')\n",
    "\n",
    "lbp_truth_n_pca, lbp_data_n_pca =  get_truth_and_data('lbp_n_pca.npy')\n",
    "lbp_truth_pca, lbp_data_pca =  get_truth_and_data('lbp_pca.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(glcm_data_n_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(0.07, 0.075, num=20)\n",
    "experiment_eps(\"glcm normal data\", arr, glcm_truth_n_pca, glcm_data_n_pca)\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  0.07404702351175586\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  2415\n",
    "# F1 score:  0.10413639094253614\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  0.07394736842105264\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  2415\n",
    "# F1 score:  0.10413639094253614"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(glcm_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(262, 271, num=10)\n",
    "experiment_eps(\"glcm data\", arr, glcm_truth_pca, glcm_data_pca)\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  268.15815815815813\n",
    "# The unique labels are:  [-1  0  1  2]\n",
    "# the amount of noisy samples are:  1483\n",
    "# F1 score:  0.09059279278913049\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  269.8198198198198\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  1419\n",
    "# F1 score:  0.09835012192322787\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  271.48148148148147\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  1406\n",
    "# F1 score:  0.09519532475473286\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  270.0\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  1417\n",
    "# F1 score:  0.09832787785740993\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chosen minpt is 0.1 of sample size  66\n",
    "the chosen eps:  253.2032032032032\n",
    "The unique labels are:  [-1  0  1  2  3]\n",
    "the amount of noisy samples are:  1917\n",
    "F1 score:  0.2662025068913859\n",
    "\n",
    "\n",
    "makes me think that perhaps the different campuses have an influence on the clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(hist_data_n_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(26, 315, num=290)\n",
    "experiment_eps(\"hist normal data\", arr, hist_truth_n_pca, hist_data_n_pca)\n",
    "\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  78.0\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  455\n",
    "# F1 score:  0.6705644983585878\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chosen minpt is 0.1 of sample size  66\n",
    "the chosen eps:  265.0\n",
    "The unique labels are:  [-1  0  1  2]\n",
    "the amount of noisy samples are:  180\n",
    "F1 score:  0.6837195748293885\n",
    "\n",
    "\n",
    "chosen minpt is 0.1 of sample size  66\n",
    "the chosen eps:  287.0\n",
    "The unique labels are:  [-1  0  1  2]\n",
    "the amount of noisy samples are:  177\n",
    "F1 score:  0.6837455473989177"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(hist_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(115, 890, num=776)\n",
    "experiment_eps(\"hist data\", arr, hist_truth_pca, hist_data_pca)\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  461.0\n",
    "# The unique labels are:  [-1  0]\n",
    "# the amount of noisy samples are:  16\n",
    "# F1 score:  0.02600561983080117\n",
    "\n",
    "# results for this are inconclusive as no more than 1 cluster forms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(hog_data_n_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(4.75, 5, num=26)\n",
    "experiment_eps(\"hog n data\", arr, hog_truth_n_pca, hog_data_n_pca)\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  4.98\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  926\n",
    "# F1 score:  0.02774458322809082\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  4.78\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  1581\n",
    "# F1 score:  0.0285036424251884\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "chosen minpt is 0.1 of sample size  66\n",
    "the chosen eps:  4.96\n",
    "The unique labels are:  [-1  0  1  2  3]\n",
    "the amount of noisy samples are:  1013\n",
    "F1 score:  0.12972889962951756"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(hog_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(3.2, 3.3, num=100)\n",
    "#arr = [3.55, 3.56, 3.57, 3.58]\n",
    "experiment_eps(\"hog data\", arr, hog_truth_pca, hog_data_pca)\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  3.2424242424242427\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  2373\n",
    "# F1 score:  0.09346536375600231\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  3.2434343434343433\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  2373\n",
    "# F1 score:  0.09346536375600231\n",
    "\n",
    "# 0.09"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(lbp_data_n_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(6,15, num=1000)\n",
    "\n",
    "experiment_eps(\"lbp data\", arr, lbp_truth_n_pca, lbp_data_n_pca)\n",
    "\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  0.6\n",
    "# The unique labels are:  [-1  0]\n",
    "# the amount of noisy samples are:  3\n",
    "# F1 score:  0.02646255238802457\n",
    "# inconclusive \n",
    "# 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_plot(lbp_data_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.linspace(0.001, 0.07, num=70)\n",
    "experiment_eps(\"lbp data\", arr, lbp_truth_pca, lbp_data_pca)\n",
    "\n",
    "# lbp data\n",
    "# chosen minpt is 0.1 of sample size  66\n",
    "# the chosen eps:  0.01\n",
    "# The unique labels are:  [-1  0  1]\n",
    "# the amount of noisy samples are:  2525\n",
    "# F1 score:  0.05038290304924796\n",
    "\n",
    "#choosen 0.01785\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the PCA section \n",
    "glcm_truth_n_pca, glcm_data_n_pca =  get_truth_and_data('GLCM_n_pca.npy')\n",
    "glcm_truth_pca, glcm_data_pca =  get_truth_and_data('GLCM_pca.npy')\n",
    "\n",
    "hist_truth_n_pca, hist_data_n_pca =  get_truth_and_data('hist_n_pca.npy')\n",
    "hist_truth_pca, hist_data_pca =  get_truth_and_data('hist_pca.npy')\n",
    "\n",
    "hog_truth_n_pca, hog_data_n_pca =  get_truth_and_data('hog_n_pca.npy')\n",
    "hog_truth_pca, hog_data_pca =  get_truth_and_data('hog_pca.npy')\n",
    "\n",
    "lbp_truth_n_pca, lbp_data_n_pca =  get_truth_and_data('lbp_n_pca.npy')\n",
    "lbp_truth_pca, lbp_data_pca =  get_truth_and_data('lbp_pca.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_r_f1 = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (9):\n",
    "    record_results(0.074, glcm_truth_n_pca, glcm_data_n_pca,  p_r_f1,   \"glcm_n_pca\" )\n",
    "    record_results(270, glcm_truth_pca, glcm_data_pca,  p_r_f1,   \"glcm_pca\" )\n",
    "\n",
    "    record_results(78, hist_truth_n_pca, hist_data_n_pca,  p_r_f1,   \"hist_n_pca\" )\n",
    "    record_results(461, hist_truth_pca, hist_data_pca,  p_r_f1,   \"hist_pca\" ) # inconclusive for this \n",
    "\n",
    "    record_results(4.78, hog_truth_n_pca, hog_data_n_pca,  p_r_f1,   \"hog_n_pca\" )\n",
    "    record_results(3.243, hog_truth_pca, hog_data_pca,  p_r_f1,   \"hog_pca\" )\n",
    "\n",
    "    record_results(6 ,lbp_truth_n_pca, lbp_data_n_pca,  p_r_f1, \"lbp_n_pca\") # inconclusive \n",
    "    record_results(0.01, lbp_truth_pca, lbp_data_pca,  p_r_f1,   \"lbp_pca\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# when sample was set to 3 \n",
    "\n",
    "record_results(0.33, glcm_truth_n_pca, glcm_data_n_pca,  p_r_f1,   \"glcm_n\" )\n",
    "record_results(480, glcm_truth_pca, glcm_data_pca,  p_r_f1,   \"glcm\" )\n",
    "record_results(180, hist_truth_n_pca, hist_data_n_pca,  p_r_f1,   \"hist_n\" )\n",
    "record_results(280, hist_truth_pca, hist_data_pca,  p_r_f1,   \"hist\" )\n",
    "\n",
    "record_results(5.033, hog_truth_n_pca, hog_data_n_pca,  p_r_f1,   \"hog_n\" )\n",
    "record_results(3.57, hog_truth_pca, hog_data_pca,  p_r_f1,   \"hog\" )\n",
    "\n",
    "#record_results(  ,lbp_truth_n_pca, lbp_data_n_pca,  p_r_f1, \"lbp_n_pca\")\n",
    "record_results(0.01785, lbp_truth_pca, lbp_data_pca,  p_r_f1,   \"lbp\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "at min sample of 3\n",
    "\n",
    "\n",
    "glcm_n\n",
    "f1 :  0.040567207796324244 \n",
    "precision:  0.8746687583522643 \n",
    "recall:  0.14823601438738807\n",
    "\n",
    "\n",
    "glcm\n",
    "f1 :  0.03785961799435621 \n",
    "precision:  0.8745252350829662 \n",
    "recall:  0.1456340399479605\n",
    "\n",
    "\n",
    "hist_n\n",
    "f1 :  0.037647995217446405 \n",
    "precision:  0.02158990220876518 \n",
    "recall:  0.1469350271676743\n",
    "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "\n",
    "\n",
    "hist\n",
    "f1 :  0.03732794525721038 \n",
    "precision:  0.5899559161743317 \n",
    "recall:  0.1439504094283309\n",
    "\n",
    "hog_n\n",
    "f1 :  0.035603995110226425 \n",
    "precision:  0.020525707436842223 \n",
    "recall:  0.13415474095048596\n",
    "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
    "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
    "\n",
    "\n",
    "hog\n",
    "f1 :  0.03290357501305513 \n",
    "precision:  0.019005339003094493 \n",
    "recall:  0.12244585597306192\n",
    "\n",
    "\n",
    "lbp\n",
    "f1 :  0.03706204445867489 \n",
    "precision:  0.5898159177581467 \n",
    "recall:  0.14234330756868446\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
